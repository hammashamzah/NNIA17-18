{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 4: Machine Learning Fundamentals & Linear Regression (Deadline: 01 Dec 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Fundamentals(7 points)\n",
    "For theoretical tasks you are encouraged to write in $\\\\LaTeX$. Jupyter notebooks support them by default. For reference, please have a look at the examples in this short excellent guide: [Typesetting Equations](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Typesetting%20Equations.ipynb)\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside the cells. But if you do this, **make sure** that the images are of high quality, so that we can read them without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Sigmoid Function (1.5 points)\n",
    "The special case of the logistic function is the *sigmoid function* which is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\sigma(a) = \\frac{1}{1 + e^{-a}}\n",
    "\\end{equation*}\n",
    "\n",
    "a) Compute its gradient analytically. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "$\\frac{\\partial}{\\partial a} \\sigma(a) = \\frac{\\partial}{\\partial a} \\frac{1}{1 + e^{-a}} = \\frac{e^{-a}}{(1 + e^{-a})^2} = 1 + \\frac{1 + e^{-a} -1}{(1 + e^{-a})^2} = \\frac{1 + e^{-a}}{(1 + e^{-a})^2} + \\frac{-1}{(1 + e^{-a})^2} = \\frac{1}{1 + e^{-a}} - (\\frac{1}{1 + e^{-a}})^2 = \\sigma(a) - \\sigma(a)^2 = \\sigma(a)(1 - \\sigma(a)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are the inherent properties that you observe from the above computed gradient? (0.5 points) <br />\n",
    "   *Hint: Think about how would the gradient signal be for the whole domain of the sigmoid function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Prove that the sigmoid function is symmetric. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Regularization (3.5 points)\n",
    "\n",
    "In the lecture, we've seen that we can add a *regularizer* to our cost function to avoid *over or underfitting*. For example, consider the following training criterion for linear regression:\n",
    "\n",
    "\\begin{equation*}\n",
    "  J(\\textbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m} \\Vert\\hat{y}^{(i)} - y^{(i)}\\Vert^{2} + \\lambda\\Omega(\\textbf{w})\n",
    "\\end{equation*}\n",
    "where $\\Omega(\\textbf{w}) = \\textbf{w}^{T}\\textbf{w}$ is the regularizer.\n",
    "\n",
    "a) In the above criterion, what is the role of the regularization parameter $\\lambda$ on the regularizer (i.e. parameters of our model) while minimizing $J(\\textbf{w})$? (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Is $\\lambda$ the model parameter or a hyperparameter? Justify.(0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Derive the closed form solution for the weights ($\\textbf{w}$) in the above criterion.(2.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Maximum Likelihood Estimation (MLE) (2 points)\n",
    "Consider the density function of a ***univariate Gaussian distribution***\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    " p(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^{2}\\right)\n",
    "\\end{equation*}\n",
    "where $\\mu$ is the $\\textit{mean}$ and $\\sigma^{2}$ is the $\\textit{variance}$. \n",
    "\n",
    "Let's say you're given *N* samples (i.e. $x_1, x_2, x_3, ..., x_N$) which are drawn from the above stated distribution. Also, you can assume that these samples are **i.i.d** (i.e. [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)).\n",
    "\n",
    "Now, please derive the *MLE step-by-step* for:\n",
    "\n",
    "a) *mean* $(\\mu)$. (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) *variance* $(\\sigma^2)$. (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression (13 points)\n",
    "\n",
    "#### 1. Introduction\n",
    "As we have seen in first assignment sheet, when we have one independent (or explanatory) variable and a scalar dependent variable, it is called **simple linear regression**.\n",
    "But, when there are more than one explanatory variable (i.e. $x^{(1)}, x^{(2)}, ...,x^{(k)}$), and a single scalar dependent variable (*y*), then it's called $\\textit{multiple linear regression}$. (Please don't confuse this with *multivariate linear regression* where we predict more than one (correlated) dependent variable.)\n",
    "\n",
    "Here, we will implement a **multiple linear regression** model in Python/NumPy using the *Gradient Descent* algorithm. Particularly, we will be using $\\textit{stochastic gradient descent}$ (*SGD*) where one performs the update step using a small set of training samples of size *batch_size* which we will set to 64. This is again a hyperparameter but in this exercise we will just use a fixed batch-size of *64* (i.e. we go through the training samples sampling 64 at a time and perform gradient descent.) Such a procedure is sometimes called *mini-batch gradient descent* in the deep learning community.\n",
    "\n",
    "Going through all the training samples *once* is called an **epoch**. Ideally, the algorithm has to go through multiple epochs over the training samples, each time shuffling it, until a convergence criterion has been satisfied. <br />\n",
    "\n",
    "Here, we will set a *tolerance value* for the difference in error (i.e. change in MSE values between subsequent epochs) that we will accept. Once this difference falls below the *tolerance value*, we terminate our training phase and return the parameters. \n",
    "\n",
    "We repeat the above training procedure for all possible hyperparameter combinations. Later on, using these parameters (*i.e. weight vectors*), we compute the prediction for validation data and the corresponding MSE values. And then, we pick the hyperparameter combination which yielded the least MSE.\n",
    "\n",
    "As a next step, we will combine training data and validation data and make it as our *new training data*. We keep the test data as it is. Using the hyperparameter combination (for the least MSE) that we found above, we train the model again with the *new training data* and obtain the parameter (*i.e. weight vector*) after convergence according to our *tolerance value*.\n",
    "\n",
    "Phew! That will be our much desired *weight vector*. This is then used on the *test data*, which has not been seen by our algorithm so far, to make a prediction. The resulting MSE value will be the so-called [*generalization error*](https://en.wikipedia.org/wiki/Generalization_error).\n",
    "\n",
    "It is this *generalization error* that we want it to be as low as possible for *unseen data* (implies that we can achieve higher accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset\n",
    "For our task, we will be using the *Wine Quality* dataset and predict the quality of white wine based on 11 features such as acidity, citric acid content, residual sugar etc. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get data\n",
    "data_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "data = pd.read_csv(data_url, sep=';')\n",
    "\n",
    "# inspect data\n",
    "#print(data.head())\n",
    "#print(data.shape)\n",
    "\n",
    "# data as np array\n",
    "data_npr = data.values\n",
    "#print (data_npr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loss function\n",
    "We will use a *regularized* form of the MSE loss function. In matrix form it can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "    J(\\textbf{w}) = \\frac{1}{2} \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert^{2} + \\frac{\\lambda}{2}\\Vert{\\textbf{w}}\\Vert^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "It's important to note that, in the above equation, $X$, called *design matrix*, is the horizontal concatenation of shape *(batch_size, num_features)* according to the *order* of the polynomial. To make things easier, you can add the *bias* term as the first column of $X$. Take care to have the *weight* vector $\\textbf{w}$ with matching dimensions.\n",
    "\n",
    "$\\textit{Hint}$: see [Design_matrix#Multiple_regression](https://en.wikipedia.org/wiki/Design_matrix#Multiple_regression) for how $X$ with 2 features looks like for $1^{st}$ degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Derive the gradient (w.r.t $\\textbf{w}$) for the regularized loss function given in **3**. (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "\\begin{equation*}\n",
    "   \\frac{\\partial}{\\partial w} J(\\textbf{w}) = \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert * X + \\lambda \\Vert{\\textbf{w}}\\Vert\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Matrix format for higher order polynomial\n",
    "\n",
    "Written in matrix form, linear regression model for second order would look like: <br />\n",
    "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + \\textbf{b}$$\n",
    "\n",
    "where $X^{2}$ is the element-wise squaring of the original design matrix $X$, $\\textbf{w}_1$ and $\\textbf{w}_2$ are the *weight* vectors, and **b** is the *bias* vector.\n",
    "\n",
    "a) Now, please write down the matrix format for a $9^{th}$ order linear regression model (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + X^{3}\\textbf{w}_{3} + X^{4}\\textbf{w}_{4} + X^{5}\\textbf{w}_{5} + X^{6}\\textbf{w}_{6} + X^{7}\\textbf{w}_{7} + X^{8}\\textbf{w}_{8} + X^{9}\\textbf{w}_{9} + \\textbf{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Hyperparameters\n",
    "we will experiment with three hyperparameters:\n",
    "\n",
    "i) regularization parameter $\\lambda$ <br />\n",
    "ii) learning rate $\\epsilon$ <br />\n",
    "iii) order of polynomial *p*\n",
    "\n",
    "And do a grid search over the values that these hyperparameters can take in order to select the best combination (i.e. the one that achieves lowest test error). This approach is called **hyperparameter optimization or tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_order = [1, 5, 9]\n",
    "learning_rates = [1e-5, 1e-8]\n",
    "lambdas = [0.1, 0.8]\n",
    "\n",
    "#hyperparams combination\n",
    "comb_gen = itertools.product(*(polynomial_order, learning_rates, lambdas))\n",
    "hparams_comb = list(comb_gen)\n",
    "#print (hparams_comb.shape)\n",
    "#print (hparams_comb)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Normalization\n",
    "First of all, inspect the data, and understand its structure and features. Ideally, before starting to train our learning algorithm, we would want the data to be normalized. Here, we normalize the data (i.e. normalize each column) using the formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "  norm\\_x_i = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
    "\\end{equation*}\n",
    "where $x_i$ is the $i^{th}$ sample in feature $x$\n",
    "\n",
    "a) Complete the following function which performs normalization (i.e. normalizes columns of $X$). (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_normalization(data):\n",
    "    # TODO: implement\n",
    "    #print (\"data shape inside data_normalization method\")\n",
    "    #print (data.shape)\n",
    "    \n",
    "    data_normalized = []\n",
    "    for column in data:\n",
    "        data_normalized_column = (column - min(column)) / (max(column) - min(column))\n",
    "        data_normalized.append(data_normalized_column)\n",
    "    #data_normalized = (data - min(data)) / (max(data) - min(data))\n",
    "    #print (type(data_normalized))\n",
    "    return np.asarray(data_normalized)\n",
    "\n",
    "#print (\"data_npr before normalization shape\")\n",
    "#print (data_npr.shape)\n",
    "# perform data normalization\n",
    "data_normalized = data_normalization(data_npr)\n",
    "data_npr = data_normalized\n",
    "#print (data_npr)\n",
    "#print (\"data_npr shape\")\n",
    "#print (data_npr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data_npr):\n",
    "    # (in-place) shuffling of data_npr along axis 0\n",
    "    np.random.shuffle(data_npr)\n",
    "\n",
    "    n_tr = 3898\n",
    "    n_va = n_tr + 500\n",
    "    n_te = n_va + 500\n",
    "    \n",
    "    X_train = data_npr[0:n_tr, 0:-1]\n",
    "    Y_train = data_npr[0:n_tr, -1]\n",
    "    \n",
    "    X_val = data_npr[n_tr:n_va, 0:-1]\n",
    "    Y_val = data_npr[n_tr:n_va, -1]\n",
    "    \n",
    "    X_test = data_npr[n_va:, 0:-1]\n",
    "    Y_test = data_npr[n_va:, -1]\n",
    "    \n",
    "    return [(X_train, Y_train), (X_val, Y_val), (X_test, Y_test)]\n",
    "\n",
    "\n",
    "# shuffle only the training data along axis 0\n",
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    \n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. Implementation of required functions\n",
    "\n",
    "Complete the following function which computes the MSE value. (0.5 point) <br />\n",
    "(i.e. just a vanilla version of it.) That is, you can ignore the regularization term and also the constants $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mse(prediction, ground_truth):\n",
    "    # TODO: implement\n",
    "    mse = np.sum((prediction - ground_truth)**2) / prediction.size\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which computes the prediction of your model. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(X, W):\n",
    "    # TODO: implement\n",
    "    #print (\"shape of training data\")\n",
    "    #print (X.shape)\n",
    "    #print (\"shape of parameters W\")\n",
    "    #print (W.shape)\n",
    "    Yhat = X.dot(W).flatten()\n",
    "    return Yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which computes the gradient of your loss function. (1.0 point) <br />\n",
    "*Hint: Just implementing the gradient computed in **3.** (a)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, Yhat, W, lambda_):\n",
    "    # TODO: implement\n",
    "    error = Y - Yhat\n",
    "    gradient = np.sum(error.dot(X)) - ((lambda_) * np.sum(W))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which performs a single update step of SGD. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: avoid in-place modification\n",
    "def sgd(gradient, lr, cur_W):\n",
    "    # TODO: implement\n",
    "    new_W = cur_W - (lr * gradient)\n",
    "    return new_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function which reformats your data as a design matrix. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X acc. to order of polynomial; likewise do it for W\n",
    "# where X is design matrix, W is the corresponding weight vector\n",
    "# [1 X X^2 X^3], [1 W1 W2 W3].T\n",
    "def prepare_data_matrix(X, W, order):\n",
    "    # TODO: implement\n",
    "    X_mat = np.ones((X.shape[0], 1))\n",
    "    \n",
    "    W_vec = [1]\n",
    "    for i in range(order):\n",
    "        X_mat = np.append(X_mat, np.power(X, order + 1), axis=1)\n",
    "        W_vec = np.append(W_vec, W, axis=0)\n",
    "    print (X_mat.shape)\n",
    "    print (W_vec.shape)\n",
    "    return X_mat, W_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. Training\n",
    "Complete the code in the following cell such that it performs **mini-batch gradient descent** on the training data for all possible hyperparameter combinations. (4.0 points)\n",
    "\n",
    "Note: You can also define a function, named appropriately, which performs training. But, take care to do correct bookkeeping of hyperparameter combinations, weight vectors, and the MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_data(data_npr)\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = itertools.chain(*splits)\n",
    "\n",
    "tolerance = 1e-3\n",
    "## different tolerance values. check how many epochs for each tolerance value\n",
    "#tolerance = 1e-4\n",
    "#tolerance = 1e-5\n",
    "#tolerance = 1e-6\n",
    "start = 1\n",
    "\n",
    "# initialize weight vector from normal distribution\n",
    "# TODO: implement\n",
    "w_shape = X_train.shape[1]\n",
    "W_init = np.random.randn(w_shape)\n",
    "# cache weights for each hyperparam combination\n",
    "# TODO: implement\n",
    "weights_hist = {key: None for key in hparams_comb}\n",
    "#print (hparams_comb)\n",
    "# keep track of MSE for each hparam combination. will be useful for plotting\n",
    "# TODO: implement\n",
    "mse_hist = {key: [] for key in hparams_comb}\n",
    "\n",
    "# find optimal hyperparameters\n",
    "for order in polynomial_order:\n",
    "    for lr in learning_rates:\n",
    "        for lamb in lambdas:\n",
    "            # initialize necessary stuffs\n",
    "            # TODO: implement\n",
    "            #iterations = 0\n",
    "            key = order, lr, lamb\n",
    "            # design matrix needed at this point\n",
    "            # use the function that we defined above\n",
    "            # TODO: implement\n",
    "            X_mat, W_vec = prepare_data_matrix(X_train, W_init, order)\n",
    "            epochs = 1\n",
    "            # goes through multiple epochs\n",
    "            while True:\n",
    "                # good idea to shuffle the train data\n",
    "                # TODO: implement\n",
    "                Xtr_shuf, Ytr_shuf = shuffle_train_data(X_mat, Y_train)\n",
    "                # some more initialization\n",
    "                # TODO: implement\n",
    "                bs = 0\n",
    "                nsamples = data_npr.shape[0]\n",
    "                # goes through 1 epoch\n",
    "                while bs < nsamples:\n",
    "                    # complete code for 1 epoch\n",
    "                    # TODO: implement\n",
    "                    tx = Xtr_shuf[bs : bs+batch_size]\n",
    "                    ty = Ytr_shuf[bs : bs+batch_size]\n",
    "                    prediction = get_prediction(tx, W_vec)\n",
    "                    gradient = compute_gradient(tx, ty, prediction, W_vec, lamb)\n",
    "                    \n",
    "                    error = compute_mse(prediction, ty)\n",
    "                    W_vec = sgd(gradient, lr, W_vec)\n",
    "                    bs += batch_size\n",
    "                \n",
    "                # after each epoch\n",
    "                # get prediction for whole X_train\n",
    "                # compute the MSE\n",
    "                # might need to do bookkeeping of mse values as well\n",
    "\n",
    "                prediction = get_prediction(X_mat, W_vec)\n",
    "                new_error = compute_mse(prediction, Y_train)\n",
    "                #new_error = get_gradient(W, X_train, Y_train)[1]\n",
    "                print (\"Epoch: %d - Error: %.4f\" %(epochs, new_error))\n",
    "                # stopping/convergence criterion\n",
    "                # check whether diff-in-mse < tolerance\n",
    "                # TODO: implement\n",
    "                if abs(new_error - error) < tolerance:\n",
    "                    print (\"Converged.\")\n",
    "                    break\n",
    "                \n",
    "    \n",
    "                # Stopping Condition\n",
    "                \n",
    "                    # cache weight vector for later use\n",
    "                    # but we also need the hparam combination\n",
    "                    # TODO: implement\n",
    "\n",
    "                    # print(\"order: {} , learning rate: {} , regularizer: {} \".format(order, lr, lamb))\n",
    "                    # print(\"Convergence after epoch {} with MSE {}\".format(epochs, ...), \"\\n\")\n",
    "                    break\n",
    "                epochs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function which selects the best hyperparameter combination (i.e. the one that gives lowest MSE on **validation data**). (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find hparams of minimum MSE on Validation data\n",
    "def find_best_hparams(weights_hist):\n",
    "    # TODO: implement\n",
    "    hpm_best, mse_best = None\n",
    "    return hpm_best, mse_best\n",
    "\n",
    "best_hpm_combination = find_best_hparams(weights_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9. Re-Training on Train + Validation data\n",
    "Complete the following function which does re-training on the combined training and validation data. (**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-run the training on X_train + X_val combined\n",
    "# Later test it on X_test; That will be our best possible MSE on test data\n",
    "# this will be more or less the same training code as you did above\n",
    "# but, here we just have only one value for each hyperparameter.\n",
    "\n",
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the convergence of MSE values using matplotlib\n",
    "# i.e. #epochs on X-axis and MSE values on Y-axis\n",
    "# TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. Evaluation on Test set\n",
    "Evaluate your model on test data. (1.0 point)\n",
    "\n",
    "**Please note that you should keep X_test undisturbed throughout this whole phase.** Else restart the kernel and start from beginning. The whole point of this exercise would not make sense if test data has been *seen in training*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finally!!!\n",
    "# test it on X_test with the Weight vector that you found above\n",
    "# this will be the generalization error of our model!!\n",
    "# TODO: implement\n",
    "\n",
    "#print(\"Finally!!! MSE achieved on X_test is : {}\".format(round(mse_test, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. Results\n",
    "Please report the following\n",
    "\n",
    "a) MSE value on Test data. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Which hyperparameter combination turned out to be the best? In your understanding, why do you think such a combination turned out to be the best for this task? (1.0 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus (2 points)\n",
    "\n",
    "Now, please repeat the whole *training, validation, re-training, and testing* procedure that we talked about above with the following hyperparameter combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polynomial_order = [1]\n",
    "learning_rates = [0.1]\n",
    "lambdas = [0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your observations during the training phase? Please explain why such a behaviour happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as the solution. The naming should include the assignment number and matriculation IDs of all members in your team in the following format:\n",
    "**assignment-4_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 members in a team). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please submit the solution to your tutor (with **[NNIA][assignment-4]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "4. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "\n",
    "Note: **If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
