{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment sheet 3: Numerical Computation and Prinicipal Component Analysis (Deadline: Nov 24, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Issues with Softmax $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture you were introduced to the softmax function which is used to generate probabilities corresponding to the output labels. Typically, the input to the softmax function is a vector of numerical values over the labels and the output is a vector(of same dimension as the input vector) of corresponding probabilities.\n",
    "**Softmax function is given by,** $~$\n",
    "$$Softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j=1}^n exp(x_j)}$$\n",
    "\n",
    "**Numerical issues might occur when computing softmax functions on a computer which can perform computations\n",
    "only upto a certain precision.** [Suggested reading $-$ [chapter 4.1 of DeepLearningBook](http://www.deeplearningbook.org/contents/numerical.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$. Name the numerical these numerical issues and explain them. ($1$ points)\n",
    "\n",
    "$2$. Suggest a remedy(with explanation on why it works) to overcome these numerical issues occuring with Softmax computation. Prove that this remedy actually does not change the softmax criteria. Describe a situation where the proposed remedy still fails to remove instability. ($1$ point)\n",
    "\n",
    "$3$. First write a naive Softmax implementation, in numpy, that can produce numerical instability. Then write a modified Softmax implementation which is numerically stable.  ($0.5 + 0.5 = 1$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "[-1.5 -0.5  0.5  1.5]\n",
      "5.0\n",
      "covariance x, y\n",
      "0.5\n",
      "[[  7 -12]\n",
      " [-12  30]]\n",
      "[[ 0.25        0.16666667]\n",
      " [ 0.16666667  1.66666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO : Define inputs\n",
    "\n",
    "def softmax_naive(inputs):\n",
    "    \"\"\"Unstable Softmax function\"\"\"\n",
    "    \n",
    "    # TODO : Implement ## underflow and overflow numerical instability\n",
    "    return np.exp(inputs)/ np.sum(np.exp(inputs))\n",
    "    pass\n",
    "\n",
    "def softmax_modified(inputs): ## inputs - max(inputs) should solve the problem\n",
    "    \"\"\"Stable Softmax function\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    return np.exp(inputs - np.max(x))/ np.sum(np.exp(inputs - np.max(inputs)))\n",
    "    pass\n",
    "\n",
    "x = np.asarray([-1, -2, -1, -1])\n",
    "y = np.asarray([1, 2, 3, 4])\n",
    "z = np.vstack((x, y))\n",
    "c = np.cov(z)\n",
    "x1 = x - (np.sum(x)/x.size)\n",
    "print (np.dot(x1, x1.T))\n",
    "y1 = y - (np.sum(y)/y.size)\n",
    "print (y1)\n",
    "print (np.dot(y1, y1.T))\n",
    "print (\"covariance x, y\")\n",
    "print (np.dot(x1, y1.T))\n",
    "print (np.dot(z, z.T))\n",
    "print (c)\n",
    "\n",
    "#a = np.array([1,2,6,4]);\n",
    "#print (np.max(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis $~$ (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Is PCA supervised or unsupervised, logically explain your answer. Which is the tunable parameter in PCA?\n",
    "Briefly explain the role of this parameter in PCA.  ($1+0.5+0.5 = 2$ points)\n",
    "\n",
    "$5.a$. Consider the following data:\n",
    "\n",
    "setA: ${\\bf x}^{(1)}$=$(2, 4)^T$, ${\\bf x}^{(2)}$=$(2, 2)^T$, ${\\bf x}^{(3)}$=$(3, 1)^T$, ${\\bf x}^{(4)}$=$(5, 1)^T$ \n",
    "\n",
    "setB: ${\\bf x}^{(1)}$=$(-1, 1)^T$, ${\\bf x}^{(2)}$=$(-2, 2)^T$, ${\\bf x}^{(3)}$=$(-1, 3)^T$, ${\\bf x}^{(4)}$=$(-1, 4)^T$\n",
    "\n",
    "Compress the above sets of vectors into a one-dimensional set using PCA, i.e., derive the encoder function $f(x)=D^{T}.x$ as defined in the lecture. Then apply f to the datasets inorder to compress them. ($1.5 + 1.5$ points)\n",
    "\n",
    "$5.b$. For both the above sets sketch the corresponding datasets in a separate figure. Also include the reconstructed vectors into the corresponding figures.                                                             ($2$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "4) PCA is unsupervised, cause PCA tries to find the hidden structure in the data by finding a new basis that maximize the variance of the data, without depending on the labels of the data.\n",
    "The tunable parameter in PCA is l, where l is less than n (n is the number of rows or columns of the covariance matrix). The parameter l, means to take the biggest l eigen values with their corresponding eigen vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"/images/pca-1\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://raw.githubusercontent.com/rudy-kh/NNIA17-18/master/assignment%203/images/pca-1.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(url=\"https://raw.githubusercontent.com/rudy-kh/NNIA17-18/master/assignment%203/images/pca-2.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent and Newton's method $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Suppose $f(x) = 2x^3 - 5x + 6$ **\n",
    "\n",
    "$6$. Write down the mathematical expressions for minimizing f(x) using Gradient descent(GD) and then using Newton's Method(NM). $~$ ($1$ points)\n",
    "\n",
    "$7$. Report the updated values of x, both for GD and NM, at $x = 0$. what do you observe? $~$ ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$8$. Perform GD and NM for the above function using Tensorflow $~$ ($1.5 + 1.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO : Implement Gradient Descent with Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO : Implement Newton's Method with Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent computation and visualisation $~$ (3 + 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now visualize the Gradient Descent algorithm to fit a straight line to a data generated using  $y = \\theta_{true}x$ $~$, i.e., use this expression to first produce the data (see code below the lines starting with m=20 and following) and then try to fit a straight line to this data. Fitting a straight line means that you have to approximate this $\\theta_{true}$ parameter using the hypothesis or predictive model by minimizing the cost function defined below.\n",
    "\n",
    "**For this task you should minimize a cost function of the form:**\n",
    "$$\\frac{1}{2m}\\sum_{i=1}^m [h_{\\theta}(x^i)-y^i]^2$$\n",
    "where,\n",
    "$x^i$ is the $i^{th}$ input \n",
    "\n",
    "$y^i$ is the true $i^{th}$ response or output\n",
    "\n",
    "$h_{\\theta}(x)$ is the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assume $~$ $h_{\\theta}(x) = \\theta x$ $~$ to be the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.02631579e-02   8.40578487e-02   7.80735001e-02   7.23101120e-02\n",
      "   6.67676845e-02   6.14462176e-02   5.63457112e-02   5.14661654e-02\n",
      "   4.68075802e-02   4.23699555e-02   3.81532914e-02   3.41575878e-02\n",
      "   3.03828449e-02   2.68290625e-02   2.34962406e-02   2.03843793e-02\n",
      "   1.74934786e-02   1.48235384e-02   1.23745588e-02   1.01465398e-02\n",
      "   8.13948136e-03   6.35338346e-03   4.78824613e-03   3.44406936e-03\n",
      "   2.32085315e-03   1.41859751e-03   7.37302440e-04   2.76967930e-04\n",
      "   3.75939850e-05   1.91806046e-05   2.21727789e-04   6.45235538e-04\n",
      "   1.28970385e-03   2.15513273e-03   3.24152217e-03   4.54887218e-03\n",
      "   6.07718275e-03   7.82645389e-03   9.79668559e-03   1.19878779e-02\n",
      "   1.44000307e-02   1.70331441e-02   1.98872180e-02   2.29622526e-02\n",
      "   2.62582477e-02   2.97752033e-02   3.35131195e-02   3.74719963e-02\n",
      "   4.16518337e-02   4.60526316e-02]\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the true data which is to be fitted\n",
    "m = 20                      # number of data points for x\n",
    "theta_true = 0.5            # corresponds to the true slope\n",
    "x = np.linspace(-1,1,m)     # x values or inputs\n",
    "y = theta_true * x          # True response\n",
    "\n",
    "\n",
    "# Create a subplot window\n",
    "# On the left window plot the true data and the approximation \n",
    "# that you obtain with different estimates of the slope theta_true\n",
    "# and on the right window plot the cost function \n",
    "\n",
    "# TODO : Create the subplot window\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,6.15))\n",
    "ax[0].scatter(x, y, marker='x', s=40, color='k')\n",
    "\n",
    "\n",
    "def hypothesis(x, theta):\n",
    "    \"\"\"Our \"hypothesis or predictive model\", a straight line through the origin.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    return x * theta\n",
    "    #pass\n",
    "\n",
    "def cost_func(theta):\n",
    "    \"\"\"The cost function describing the goodness of fit.\"\"\"  \n",
    "    \n",
    "    # TODO : Implement\n",
    "    #pred = hypothesis(x, theta)\n",
    "    #m = pred.size\n",
    "    #y1 = y.reshape(y.size, 1)\n",
    "    #sq_errors = (pred - y1)**2\n",
    "    #return (1.0/(2 * m)) * sq_errors.sum()\n",
    "    theta = np.atleast_2d(np.asarray(theta))\n",
    "    return np.average((y-hypothesis(x, theta))**2, axis=1)/2\n",
    "    #pass\n",
    "\n",
    "\n",
    "# First construct a grid of theta parameter and their corresponding\n",
    "# cost function values.\n",
    "theta_grid = np.linspace(-0.2,1,50)\n",
    "#print (theta_grid)\n",
    "#print (theta_grid.shape)\n",
    "#cost_grid = cost_func(theta_grid[:, np.newaxis])\n",
    "#print (theta_grid.reshape(theta_grid.size, 1))\n",
    "#print (cost_grid)\n",
    "# Find the cost function values to be stored in J_grid\n",
    "# TODO : Create J_grid\n",
    "J_grid = cost_func(theta_grid[:, np.newaxis])\n",
    "#print (J_grid)\n",
    "\n",
    "\n",
    "# Plot the cost function as a function of theta.\n",
    "# TODO : Do the plot\n",
    "ax[1].plot(theta_grid, J_grid, 'k')\n",
    "\n",
    "# Take N steps with learning rate alpha down the steepest gradient,\n",
    "# starting at theta = 0.\n",
    "N = 10\n",
    "alpha = 1 \n",
    "# this is just a starting value of alpha, \n",
    "# you must consider different values of alpha (try using large values)\n",
    "# and redo the steps below to generate different plots\n",
    "theta = [0]\n",
    "J = [cost_func(theta[0])[0]]\n",
    "for j in range(N-1):\n",
    "    last_theta = theta[-1]\n",
    "    this_theta = last_theta - alpha / m * np.sum(\n",
    "                                    (hypothesis(x, last_theta) - y) * x)\n",
    "    theta.append(this_theta)\n",
    "    J.append(cost_func(this_theta))\n",
    "\n",
    "# Compute the N steps down the steepest gradient\n",
    "# TODO\n",
    "\n",
    "# Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window of the subplot in a matching colour.\n",
    "# TODO\n",
    "\n",
    "# Put the labels, titles and a legend.\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now assume that the data is generated using  $y = \\theta_1x + \\theta_0$\n",
    "** Following the same logic you applied for the above task define a predictive model \n",
    "and perform 5 steps of gradient descent with learning rate alpha = 0.7 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the true data which is to be fitted\n",
    "m = 20\n",
    "theta0_true = 2\n",
    "theta1_true = 0.5\n",
    "x = np.linspace(-1,1,m)\n",
    "y = theta0_true + theta1_true * x\n",
    "\n",
    "# Create the sub-plot: left window is the data, right window will be the cost function.\n",
    "# TODO\n",
    "\n",
    "\n",
    "def hypothesis(x, theta0, theta1):\n",
    "    \"\"\"Our \"hypothesis function\", a straight line.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    return theta1 * x + theta0\n",
    "    pass\n",
    "\n",
    "def cost_func(theta0, theta1):\n",
    "    \"\"\"The cost function, J(theta0, theta1) describing the goodness of fit.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "\n",
    "# First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "# corresponding cost function values.\n",
    "theta0_grid = np.linspace(-1,4,101)\n",
    "theta1_grid = np.linspace(-5,5,101)\n",
    "# Compute the cost function values\n",
    "# TODO\n",
    "\n",
    "\n",
    "# Do a labeled contour plot for the cost function on right window of the above subplot\n",
    "# TODO \n",
    "\n",
    "\n",
    "# Take 5 steps with learning rate alpha = 0.7 down the steepest gradient,\n",
    "# starting at (theta0, theta1) = (0, 0).\n",
    "# TODO\n",
    "\n",
    "\n",
    "# Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window in a matching colour.\n",
    "# TODO\n",
    "\n",
    "\n",
    "# Put the labels, titles and a legend.\n",
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Bonus\n",
    "***[Additional material - Linear Algebra Basics](http://www.cs.ubc.ca/~schmidtm/Documents/2009_Notes_LinearAlgebra.pdf)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace of a Matrix $~$ (3 points)\n",
    "***[Reading material on Trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that the trace of a ***symmetric positive definite*** matrix is the sum of its eigenvalues.    ($0.5$ points)\n",
    "\n",
    "Suppose Y is a mxn matrix with ***$m \\leq n$ and has full rank***, then\n",
    "\n",
    "a.   Give the rank of Y.                                                                 ($0.5$ points)\n",
    "\n",
    "b.  Show that trace of $Y^{T}(Y^TY)^{-1}Y$ = rank(Y)                                     ($1$ points)\n",
    "\n",
    "c. Prove that $Y^{T}(Y^TY)^{-1}Y$ is the projection matrix w.r.t space defined by Y.     ($1$ points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian $~$ (3 points)\n",
    "\n",
    "***[Reading material on Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the Jacobian determinant of $\\frac{\\partial(fg, h)}{\\partial(u, v)}$ is equal to $\\frac{\\partial(f, h)}{\\partial(u, v)}g + f\\frac{\\partial(g, h)}{\\partial(u, v)}$,\n",
    "\n",
    "where f,g,h are functions of u and v(i.e., f(u,v), g(u,v), h(u,v))   ($3$ points)\n",
    "\n",
    "Hint: Use the property $\\frac{\\partial(y, x)}{\\partial(u, v)} = \\frac{\\partial(y)}{\\partial(u)}\\frac{\\partial(x)}{\\partial(v)}-\\frac{\\partial(y)}{\\partial(v)}\\frac{\\partial(x)}{\\partial(u)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial(fg, h)}{\\partial(u, v)} = \\frac{\\partial(fg)}{\\partial(u)}\\frac{\\partial(h)}{\\partial(v)}-\\frac{\\partial(fg)}{\\partial(v)}\\frac{\\partial(h)}{\\partial(u)}$ (1)\n",
    "\n",
    "### solution\n",
    "\n",
    "now $\\frac{\\partial(fg)}{\\partial(u)} = f \\frac{\\partial(g)}{\\partial(u)} + g \\frac{\\partial(g)}{\\partial(u)} $\n",
    "and $\\frac{\\partial(fg)}{\\partial(v)} = f \\frac{\\partial(g)}{\\partial(v)} + g \\frac{\\partial(g)}{\\partial(v)} $ (2)\n",
    "\n",
    "By substituting (2) in (1) we get : \n",
    "\n",
    "$\\frac{\\partial(fg, h)}{\\partial(u, v)} =(f \\frac{\\partial(g)}{\\partial(u)} + g \\frac{\\partial(g)}{\\partial(u)})\\frac{\\partial(h)}{\\partial(v)} - (f \\frac{\\partial(g)}{\\partial(v)} + g \\frac{\\partial(g)}{\\partial(v)}) \\frac{\\partial(h)}{\\partial(u)} = f (\\frac{\\partial(g)}{\\partial(u)}\\frac{\\partial(h)}{\\partial(v)} - \\frac{\\partial(g)}{\\partial(v)}\\frac{\\partial(h)}{\\partial(u)}) + g (\\frac{\\partial(f)}{\\partial(u)}\\frac{\\partial(h)}{\\partial(v)} - \\frac{\\partial(f)}{\\partial(v)}\\frac{\\partial(h)}{\\partial(u)}) = f \\frac{\\partial(g, h)}{\\partial(u, v)} + g \\frac{\\partial(f, h)}{\\partial(u, v)}$ \n",
    "\n",
    "Q.E.D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian $~$ (2 points)\n",
    "***[Reading material on Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$M=\\left[\\begin{array}{cccc}\n",
    "   5 & 1 & 0 & 1\\\\\n",
    "   1 & 4 & 1 & 0\\\\\n",
    "   0 & 1 & 3 & 1\\\\\n",
    "   1 & 0 & 1 & 2\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "denote the Hessian matrix, a particular point, for a functional.\n",
    "\n",
    "a. What properties of the functional can you infer from the above information.(give mathematical reasons) ($1$ point)\n",
    "\n",
    "b. Provide a generic mathematical representation(e.g. generic representation of a straight line is $ax+by+c=0$) for the above functional. ($1$ point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
